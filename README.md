# OAI Reverse Proxy

Reverse proxy server for the OpenAI and Anthropic APIs. Forwards text generation requests while rejecting administrative/billing requests. Includes optional rate limiting and prompt filtering to prevent abuse.

### Table of Contents
- [What is this?](#what-is-this)
- [Why?](#why)
- [Usage Instructions](#setup-instructions)
  - [Deploy to Huggingface (Recommended)](#deploy-to-huggingface-recommended)
  - [Deploy to Repl.it (WIP)](#deploy-to-replit-wip)
- [Local Development](#local-development)
- [Custom Page](#custom-page)
- [Credits](#Credits)
## What is this?
If you would like to provide a friend access to an API via keys you own, you can use this to keep your keys safe while still allowing them to generate text with the API. You can also use this if you'd like to build a client-side application which uses the OpenAI or Anthropic APIs, but don't want to build your own backend. You should never embed your real API keys in a client-side application. Instead, you can have your frontend connect to this reverse proxy and forward requests to the downstream service.

This keeps your keys safe and allows you to use the rate limiting and prompt filtering features of the proxy to prevent abuse.

## Why?
OpenAI keys have full account permissions. They can revoke themselves, generate new keys, modify spend quotas, etc. **You absolutely should not share them, post them publicly, nor embed them in client-side applications as they can be easily stolen.**

This proxy only forwards text generation requests to the downstream service and rejects requests which would otherwise modify your account. 

---

## Usage Instructions
If you'd like to run your own instance of this proxy, you'll need to deploy it somewhere and configure it with your API keys. A few easy options are provided below, though you can also deploy it to any other service you'd like.

### Deploy to Huggingface (Recommended)
[See here for instructions on how to deploy to a Huggingface Space.](./docs/deploy-huggingface.md)

### Deploy to Render
[See here for instructions on how to deploy to Render.com.](./docs/deploy-render.md)

## Local Development
To run the proxy locally for development or testing, install Node.js >= 18.0.0 and follow the steps below.

1. Clone the repo
2. Install dependencies with `npm install`
3. Create a `.env` file in the root of the project and add your API keys. See the [.env.example](./.env.example) file for an example.
4. Start the server in development mode with `npm run start:dev`.

You can also use `npm run start:dev:tsc` to enable project-wide type checking at the cost of slower startup times. `npm run type-check` can be used to run type checking without starting the server.

## Custom Page

Create PAGE_BODY inside your secrets or .env file, and add html code encoded as Base64.

Inside html various replacments will happen with these {tags}:

<details><summary>List of all tags</summary>

{JSON} - Shows json of all data.

{headerHtml} - Original header. (will be removed)

{uptime} - Current uptime since restart in seconds.

{title} - Tilte of the page/bookmark.

{endpoints:openai} - Link to the endpoint for OpenAI Api.

{endpoints:anthropic} - Link to the endpoint for Anthropic Api.

{proompts} - Number of total propmts generated by users.

{proomptersNow} - Number of online users.

{openaiKeys} - Amount of OpenAI keys.

{anthropicKeys} - Amount of Anthropic keys.

{status} - Current status (Checking keys)

{anthropic:activeKeys} - How many active Anthropic keys are available. 

{anthropic:proomptersInQueue}  How many people are in Anthropic request queue.

{anthropic:estimatedQueueTime} - How much estimated time it will take for person in Anthropic queue to be processed. 

{turbo:activeKeys} - How many active Openai keys are turbo.

{turbo:overQuotaKeys} - How many over quota keys are GPT-4

{turbo:revokedKeys} - How many revoked keys are GPT-4

{turbo:proomptersInQueue} - How many people are in Turbo request queue.

{turbo:estimatedQueueTime} - How much estimated time it will take for person in Turbo queue to be processed. 

{gpt4:activeKeys} - How many active Openai keys are GPT-4.

{gpt4:overQuotaKeys} - How many over quota keys are GPT-4

{gpt4:revokedKeys} - How many revoked keys are GPT-4

{gpt4:proomptersInQueue} - How many people are in GPT-4 request queue.

{gpt4:estimatedQueueTime} - How much estimated time it will take for person in GPT-4 queue to be processed. 

{gpt432k:activeKeys} - How many active Openai keys are GPT-4 32K 

{gpt432k:overQuotaKeys} - How many over quota keys are GPT-4 32K

{gpt432k:revokedKeys} - How many revoked keys are GPT-4 32K

{config:page_body} - Don't use, it will show full raw html.

{config:gatekeeper} - Shows gatekeeper set.

{config:modelRateLimit} - Shows Rate limit for all models.

{config:maxContextTokensOpenAI} - Shows max context tokens for OpenAI models.

{config:maxContextTokensAnthropic} - Shows max context tokens for Anthropic models.

{config:maxOutputTokensOpenAI} - Shows max output tokens for OpenAI models.

{config:maxOutputTokensAnthropic} - Shows max output tokens for Anthropic models.

{config:rejectDisallowed} - Read config (don't remember)

{config:rejectMessage} - Read config (don't remember)

{config:promptLogging} - Shows if proxy is logged (Prompt logging will be soon completly removed)

{config:queueMode} - Shows what queue mode is set.

{config:turboOnly} -  Shows if it's forced to turbo only mode.

{user:data} - Shows user data array (used in js script for leaderboards)

{user:aliases} - Shows user aliases array (used in js script for leaderboards)

{build} - Shows current build name.
</details>

## Credits 

| **Hmage** | - his code helped me implement "Support for multiple organizations per key" (https://gitgud.io/afurry/goai/-/commit/342ba44d41d6fad94799b048552ef7b8187d81ae)



